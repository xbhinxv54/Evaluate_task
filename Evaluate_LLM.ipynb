{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -U -q \"google-generativeai>=0.8.3\""
      ],
      "metadata": {
        "id": "KOU5MIYGcLdc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "5nCKdjtHcQsb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY ='AIzaSyDZvooh6zAA9XV97ZWT05SX4uwRgn8SZaw'"
      ],
      "metadata": {
        "id": "77uztZadcY1y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "_1gc5Be8dfs5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flash= genai.GenerativeModel('gemini-1.5-flash')\n",
        "judge = flash.start_chat(history=[])"
      ],
      "metadata": {
        "id": "ptkoHfE8dkn_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"The company launched its new software with features like data encryption and automatic backups. Pricing information is available, but there's no mention of customer support options\"\n",
        "prompt=f\"\"\"You are given a task to judge two LLM models. The metrics for which should be:\n",
        "    1. Accuracy: Whether the output matches the expected output\n",
        "    2. Explanation Quality (if provided): How relevant and logical is the reasoning\n",
        "    3. Ambiguity Handling: How well the LLM deals with unclear or implied information\n",
        "    4. Consistency: Are the answers from each LLM consistent with their reasoning (if provided)?\n",
        "    5. Reliability: Are the outputs repeatable across multiple runs?\n",
        "    Make a decision only if you are completely sure ,reply as Marked for Human Reveiw\n",
        "    Context Paragraph: {text}\"\"\"\n",
        "judge.send_message(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "VCrpESB4ikmc",
        "outputId": "30b90031-6f11-4be7-a43a-c02ffad511c1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "response:\n",
              "GenerateContentResponse(\n",
              "    done=True,\n",
              "    iterator=None,\n",
              "    result=protos.GenerateContentResponse({\n",
              "      \"candidates\": [\n",
              "        {\n",
              "          \"content\": {\n",
              "            \"parts\": [\n",
              "              {\n",
              "                \"text\": \"Marked for Human Review\\n\"\n",
              "              }\n",
              "            ],\n",
              "            \"role\": \"model\"\n",
              "          },\n",
              "          \"finish_reason\": \"STOP\",\n",
              "          \"avg_logprobs\": -0.00016223494894802571\n",
              "        }\n",
              "      ],\n",
              "      \"usage_metadata\": {\n",
              "        \"prompt_token_count\": 158,\n",
              "        \"candidates_token_count\": 5,\n",
              "        \"total_token_count\": 163\n",
              "      }\n",
              "    }),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def score(model1, model2, truth):\n",
        "\n",
        "    full_prompt = f\"\"\"\n",
        "    Here is the question to which the model is answering:{question}\n",
        "    Model 1 Answer: {model1}\n",
        "    Model 2 Answer: {model2}\n",
        "    Ground Truth Answer: {truth}\n",
        "\n",
        "    Please provide a detailed evaluation for each model across the specified metrics:\n",
        "    - Assign a score from 0-10 for each metric\n",
        "    - Compare the two models' performances\n",
        "    - Recommend which model performs better and why\"\"\"\n",
        "    judge.send_message(full_prompt)"
      ],
      "metadata": {
        "id": "n91-O5_rdqOb"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1=genai.GenerativeModel('gemini-1.0-pro')\n",
        "model2=genai.GenerativeModel('gemini-1.0-pro')"
      ],
      "metadata": {
        "id": "CjGdUXElfN45"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat1 = model1.start_chat(history=[])\n",
        "chat2=model2.start_chat(history=[])\n"
      ],
      "metadata": {
        "id": "ksPUZ7WCf4Fr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result1=[]\n",
        "result2=[]\n",
        "questions=[\"Does it mention pricing?\",\"Is customer support discussed?\",\"Does it talk about data encryption?\"]\n",
        "\n",
        "for question in questions:\n",
        "\n",
        "  prompt=f\"\"\"Here is a paragraph\n",
        "  \"The company launched its new software with features like data encryption and automatic backups.\n",
        "  Pricing information is available, but there's no mention of customer support options\"\n",
        "  Answer the following {question} based on the paragraph. The response should be Yes or No\n",
        "  \"\"\"\n",
        "  r1=chat1.send_message(prompt)\n",
        "  r2=chat1.send_message(prompt)\n",
        "  result1.append(r1.text)\n",
        "  result2.append(r2.text)"
      ],
      "metadata": {
        "id": "58oZ1Qm8gMjn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9FRqUiegzsv",
        "outputId": "61732dde-22bd-47b1-8abf-c3f141787b71"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Yes', 'No', 'Yes']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "truth=[\"Yes\",\"No\",\"Yes\"]\n",
        "for i,j,k, in zip(result1,result2,truth):\n",
        "  score(i,j,k)\n"
      ],
      "metadata": {
        "id": "lR92PgIohUcR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verdict=judge.send_message(\"which model is better\")\n",
        "print(verdict.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "tHObugHWjbVv",
        "outputId": "a720b769-331e-405f-cec0-3ead9e9c9707"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based solely on the provided examples, neither model is demonstrably \"better.\"  They both succeed or fail equally on the simple yes/no questions about data encryption.  To determine which model is superior, a much more extensive evaluation is needed, encompassing:\n",
            "\n",
            "* **More diverse questions:**  Include questions requiring more nuanced understanding and information extraction.\n",
            "* **Multiple runs:** Evaluate the consistency and reliability of the models across multiple trials with the same input.\n",
            "* **Questions requiring explanations:** Assess the quality and logic of the models' reasoning processes.\n",
            "* **Ambiguous prompts:** Test how each model handles incomplete or unclear information.\n",
            "\n",
            "Only after such a thorough evaluation could a well-informed judgment be made about which model is superior.  The current data provides only a very limited view of the models' capabilities.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nf3phu9sjq3X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}